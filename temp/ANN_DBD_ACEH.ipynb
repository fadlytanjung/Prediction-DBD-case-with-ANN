{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.preprocessing import Imputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import SGD, Adagrad, RMSprop, Adadelta, Adamax, Adam\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import math\n",
    "\n",
    "Imputer = SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self):\n",
    "        self.trainScore = 0\n",
    "        self.testScore = 0\n",
    "        self.rmseTrain = 0\n",
    "        self.rmseTest = 0\n",
    "\n",
    "    #load data input\n",
    "    def load_data(self, file):\n",
    "        data = pd.read_excel(file)\n",
    "        return data.values\n",
    "    \n",
    "    def sum_data(self, data):\n",
    "        df = pd.DataFrame(data)\n",
    "        new_data = np.zeros(4, )\n",
    "        new_data = data[:,[1,2]]\n",
    "        return new_data\n",
    "    \n",
    "    #hapus kolom yang tidak digunakan\n",
    "    def hapus_kolom(self, data, kolom):\n",
    "        return np.delete(data, kolom, axis=1)\n",
    "    \n",
    "    def one_hot(self, data):\n",
    "        values = data[:, 1]\n",
    "        # define example\n",
    "        #encode kecamatan ke integer\n",
    "        # integer encode\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        integer_encoded = self.label_encoder.fit_transform(values)\n",
    "        data[:, 1] = integer_encoded\n",
    "        return data\n",
    "    \n",
    "    def isi_kolom_kosong(self,data):\n",
    "        values = data[:]\n",
    "        self.imp = Imputer(missing_values=np.NAN, strategy='mean', fill_value=None, verbose=0, copy=True)\n",
    "        fill_column = self.imp.fit_transform(values)\n",
    "        data[:] = fill_column\n",
    "        return data\n",
    "    \n",
    "    def split_data(self, data):\n",
    "        #split data into X and Y variables\n",
    "        X = data[:,[1,2,3,4,5,6,7,8]]\n",
    "        y = data[:,0]\n",
    "        y = y.reshape(-1,1)\n",
    "        return X, y\n",
    "    \n",
    "    def normalisasi(self, data):\n",
    "        self.scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        y_scaled = self.scaler.fit_transform(y)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size = 0.2)\n",
    "        return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN:\n",
    "    def __init__(self):\n",
    "        self.model_loaded = False\n",
    "\n",
    "    def set_param(self, neuron=4, optimizer=\"Adam\", epoch=10, batch_size=1, lr=0.001, activation='relu'):\n",
    "        self.neuron = neuron\n",
    "        self.optimizer = optimizer\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.model_loaded = False\n",
    "        self.learning_rate = lr\n",
    "        self.activation = activation\n",
    "\n",
    "    def soft_acc(self, y_true, y_pred):\n",
    "        return K.mean(K.equal(K.round(y_true), K.round(y_pred)))\n",
    "    \n",
    "        '''pembuatan model'''\n",
    "    def training(self, X_train, X_test, y_train, y_test, preprocessing):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(4, input_dim=8, activation=self.activation))  # inputlayer\n",
    "        model.add(Dense(self.neuron, activation=self.activation))  # hiddenlayer\n",
    "        model.add(Dense(1, activation='linear'))  # outputlayer\n",
    "        if 'SGD' in self.optimizer:\n",
    "            opt = SGD(lr=0.001)\n",
    "            \n",
    "        if 'RMSProp' in self.optimizer:\n",
    "            opt = RMSprop(lr=0.001)\n",
    "\n",
    "        if 'Adgrad' in self.optimizer:\n",
    "            opt = Adgrad(lr=0.001)\n",
    "            \n",
    "        if 'Adamax' in self.optimizer:\n",
    "            opt = Adamax(lr=0.001)\n",
    "\n",
    "        if 'Adam' in self.optimizer:\n",
    "            opt = Adam(lr=0.001)\n",
    "\n",
    "        if 'Adadelta' in self.optimizer:\n",
    "            opt = Adadelta(lr=0.001)\n",
    "            \n",
    "        model.compile(loss='mean_squared_error', optimizer=opt)\n",
    "        self.history = model.fit(X_train, y_train, epochs=self.epoch, batch_size=self.batch_size, verbose=2, validation_data=(X_test,y_test))\n",
    "\n",
    "        # save history\n",
    "        loss_history = self.history.history[\"loss\"]\n",
    "        # acc_history = self.history.history[\"soft_acc\"]\n",
    "        testing_loss_history = self.history.history[\"val_loss\"]\n",
    "        # testing_acc_history = self.history.history[\"val_soft_acc\"]\n",
    "        loss = np.array(loss_history)\n",
    "        np.savetxt(\"static/loss_history.txt\", loss, delimiter=\",\")\n",
    "        # acc = np.array(acc_history)\n",
    "        # np.savetxt(\"static/acc_history.txt\", acc, delimiter=\",\")\n",
    "        tes_loss = np.array(testing_loss_history)\n",
    "        np.savetxt(\"static/testing_loss_history.txt\", tes_loss, delimiter=\",\")\n",
    "        # tes_acc = np.array(testing_acc_history)\n",
    "        # np.savetxt(\"static/testing_acc_history.txt\", tes_acc, delimiter=\",\")\n",
    "        \n",
    "        model_json = model.to_json()\n",
    "        with open(\"model.json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "\n",
    "        model.save_weights('weights.h5')\n",
    "        \n",
    "        testPredict = model.predict(X_test)\n",
    "        testPredict = preprocessing.scaler.inverse_transform(testPredict)\n",
    "        \n",
    "        # Estimate model performance\n",
    "        trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "        print('Train Score: %.5f MSE (%.5f RMSE)' % (trainScore, math.sqrt(trainScore)))\n",
    "        testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "        print('Test Score: %.5f MSE (%.5f RMSE)' % (testScore, math.sqrt(testScore)))\n",
    "        self.trainScore = trainScore\n",
    "        self.testScore = testScore\n",
    "        self.rmseTrain = math.sqrt(trainScore)\n",
    "        self.rmseTest = math.sqrt(testScore)\n",
    "        score = np.array([self.trainScore,self.testScore,self.rmseTrain,self.rmseTest]);\n",
    "        np.savetxt(\"static/score.txt\",score, delimiter=\";\")\n",
    "        \n",
    "        print(X_test[0])\n",
    "        # plot baseline and predictions\n",
    "#         X_test = preprocessing.scaler.inverse_transform(X_test[:,0])\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_predict_sample_orig = preprocessing.scaler.inverse_transform(y_pred)\n",
    "        y_test = preprocessing.scaler.inverse_transform(np.reshape(y_test,(-1,1)))\n",
    "        kecamatan_asli = preprocessing.label_encoder.fit_transform(X_test[:,0])\n",
    "        df = pd.DataFrame({'Kecamatan': kecamatan_asli.flatten(),'Aktual': y_test.flatten(), 'Prediksi': y_predict_sample_orig.flatten()})\n",
    "        writer = pd.ExcelWriter('static/hasil_training.xlsx', engine='xlsxwriter')\n",
    "        df.to_excel(writer, \"Sheet1\")\n",
    "        writer.save()\n",
    "        K.clear_session()\n",
    "        \n",
    "    def load_model(self):\n",
    "        # load json file\n",
    "        json_file = open(\"model.json\", \"r\")\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "\n",
    "        # load weight\n",
    "        self.model = model_from_json(loaded_model_json)\n",
    "        self.model.load_weights(\"weights.h5\")\n",
    "\n",
    "        self.model.compile(loss='mean_squared_error', optimizer=self.optimizer, metrics=[self.soft_acc])\n",
    "        self.model_loaded = True\n",
    "        \n",
    "    def prediction(self, datax, bulan, start_bulan, start_tahun, preprocessing):\n",
    "        if self.model_loaded == False:\n",
    "            print(\"Model loaded\")\n",
    "            self.load_model()\n",
    "\n",
    "        tulis = np.zeros((4,), dtype=\"S250\")\n",
    "        '''prediksi sebanyak variabel bulan'''\n",
    "        for bln in range(bulan):\n",
    "            # make predictions\n",
    "            '''masukkan hasil prediksi ke feature untuk ditabelin'''\n",
    "            predict = self.model.predict(datax)\n",
    "            predict = preprocessing.scaler.inverse_transform(np.reshape(predict,(-1,1)))\n",
    "            new_data = np.zeros(4, )\n",
    "            x = 0\n",
    "            for i in datax:\n",
    "                temp = np.array((start_bulan,start_tahun,i[0]))\n",
    "                new_data = np.vstack((new_data, np.append(temp, predict[x])))\n",
    "                x += 1\n",
    "            new_data = np.delete(new_data, 0, axis=0)\n",
    "\n",
    "            result_test = new_data  # preprocessing.scaler.inverse_transform(new_data)\n",
    "            result_test = np.rint(result_test)\n",
    "            kecamatan = result_test[:, 0]\n",
    "            kecamatan = kecamatan.astype(int)\n",
    "            kecamatan_asli = preprocessing.label_encoder.inverse_transform(kecamatan)\n",
    "\n",
    "            start_bulan += 1\n",
    "            if start_bulan>12:\n",
    "                start_tahun+=1\n",
    "                start_bulan=1\n",
    "\n",
    "            tampilkan = result_test\n",
    "            tampilkan = tampilkan.astype(\"S250\")\n",
    "            tampilkan[:, 0] = kecamatan_asli\n",
    "#             # tampilkan[:,0] = start_bulan\n",
    "#             df = pd.DataFrame({'Bulan ke': result_test[:, 0].flatten(),'Tahun': result_test[:,1].flatten(),'Kecamatan': kecamatan_asli.flatten(),'Jumlah Kasus': predict.flatten()})\n",
    "            df = pd.DataFrame(tampilkan)\n",
    "            tulis = np.vstack((tulis, tampilkan))\n",
    "            print(\"yang ke-\", bln)\n",
    "            df.columns = ['Bulan ke','Tahun','Kecamatan', 'Jumlah Kasus']\n",
    "            # print(df)\n",
    "\n",
    "            # result_test[:, 0] = start_bulan\n",
    "            # result_test[:,1] = start_tahun\n",
    "            datax = result_test[:,2:4]\n",
    "            #print(\"bulan\")\n",
    "            #print(datax)\n",
    "            datax = preprocessing.normalisasi(datax)\n",
    "\n",
    "        tulis = np.delete(tulis, 0, axis=0)\n",
    "        df = pd.DataFrame(tulis)\n",
    "        df.columns = ['Bulan ke','Tahun','Kecamatan', 'Jumlah Kasus']\n",
    "\n",
    "        writer = pd.ExcelWriter('static/prediksi.xlsx', engine='xlsxwriter')\n",
    "        df.to_excel(writer, \"Sheet1\")\n",
    "        writer.save()\n",
    "        K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.28571415 0.8399999  0.11769734 0.28333333 0.\n",
      " 0.515358   0.04144909]\n",
      "Epoch 1/10\n",
      "27/27 - 0s - loss: 0.0585 - val_loss: 0.0469\n",
      "Epoch 2/10\n",
      "27/27 - 0s - loss: 0.0291 - val_loss: 0.0322\n",
      "Epoch 3/10\n",
      "27/27 - 0s - loss: 0.0229 - val_loss: 0.0262\n",
      "Epoch 4/10\n",
      "27/27 - 0s - loss: 0.0187 - val_loss: 0.0221\n",
      "Epoch 5/10\n",
      "27/27 - 0s - loss: 0.0161 - val_loss: 0.0191\n",
      "Epoch 6/10\n",
      "27/27 - 0s - loss: 0.0145 - val_loss: 0.0173\n",
      "Epoch 7/10\n",
      "27/27 - 0s - loss: 0.0136 - val_loss: 0.0163\n",
      "Epoch 8/10\n",
      "27/27 - 0s - loss: 0.0131 - val_loss: 0.0156\n",
      "Epoch 9/10\n",
      "27/27 - 0s - loss: 0.0126 - val_loss: 0.0152\n",
      "Epoch 10/10\n",
      "27/27 - 0s - loss: 0.0123 - val_loss: 0.0148\n",
      "Train Score: 0.01211 MSE (0.11007 RMSE)\n",
      "Test Score: 0.01475 MSE (0.12146 RMSE)\n",
      "[0.         0.28571415 0.8399999  0.11769734 0.28333333 0.\n",
      " 0.515358   0.04144909]\n",
      "Model loaded\n",
      "yang ke- 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 432, 108, 432, 108\nPlease provide data which shares the same first dimension.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-61ef81142d39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2019\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-0ab1b7d89fd9>\u001b[0m in \u001b[0;36mprediction\u001b[0;34m(self, datax, bulan, start_bulan, start_tahun, preprocessing)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;31m# make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;34m'''masukkan hasil prediksi ke feature untuk ditabelin'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m     87\u001b[0m           method.__name__))\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1247\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m           model=self)\n\u001b[0m\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m             label, \", \".join(str(i.shape[0]) for i in nest.flatten(data)))\n\u001b[1;32m    281\u001b[0m       \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Please provide data which shares the same first dimension.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 432, 108, 432, 108\nPlease provide data which shares the same first dimension."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # fix random seed for reproducibility#\n",
    "    np.random.seed(11)\n",
    "    preprocessing = Preprocessing()\n",
    "\n",
    "    data = preprocessing.load_data(\"data/DBD_Bulan_Aceh.xlsx\")\n",
    "    data = preprocessing.hapus_kolom(data, [0, 1])\n",
    "    data = preprocessing.one_hot(data)\n",
    "    data = preprocessing.isi_kolom_kosong(data)\n",
    "    data = data.astype('float32')\n",
    "    X, y = preprocessing.split_data(data)\n",
    "    X_train, X_test, y_train, y_test = preprocessing.normalisasi(data)\n",
    "    print(X_test[0])\n",
    "    nn = ANN()\n",
    "    nn.set_param(neuron=8, optimizer=\"Adam\", epoch=10, batch_size=16)\n",
    "    nn.training(X_train, X_test, y_train, y_test, preprocessing)\n",
    "    \n",
    "    nn.prediction(X_test, 6, 1, 2019, preprocessing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
